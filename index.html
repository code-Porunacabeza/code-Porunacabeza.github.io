
<html><head lang="en"><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
  <meta http-equiv="x-ua-compatible" content="ie=edge">

  <title>GAZER</title>

  <meta name="description" content="">
  <meta name="viewport" content="width=device-width, initial-scale=1">

<link rel="icon" type="image/png" href="./index_files/icon.png">
  <link rel="stylesheet" href="./index_files/bootstrap.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="./index_files/codemirror.css">
  <link rel="stylesheet" href="./index_files/app.css">
  <!-- <link rel="stylesheet" href="./index_files/bootstrap.min(1).css"> -->

  <script type="text/javascript" async="" src="./index_files/analytics.js"></script>
  <script type="text/javascript" async="" src="./index_files/analytics(1).js"></script>
  <script async="" src="./index_files/js"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-110862391-3');
  </script>

  <script src="./index_files/jquery.min.js"></script>
  <script src="./index_files/bootstrap.min.js"></script>
  <script src="./index_files/codemirror.min.js"></script>
  <script src="./index_files/clipboard.min.js"></script>

  <script src="./index_files/app.js"></script>
</head>

<body data-gr-c-s-loaded="true">
  <div class="container" id="main">
      <div class="row">
          <h1 class="col-md-12 text-center">
              <!-- <img src="./index_files/images/name.png" class="img-responsive" alt="overview"><br> -->
              <strong >GAZER</strong> : Model-active human-centered artificial intelligence for medical visual assistance
          <br /><br />
          <!-- <small><strong style="color:rgb(204,0,0);">
              [Accepted by CVPR 2023]
          </strong></small> -->
          <!-- <br /><br /> -->
          </h1>
      </div>
      

      <div class="row">
          <div class="col-md-8 col-md-offset-2 text-center">
              <ul class="nav nav-pills nav-justified">
                  <li>
                      <a href="./index.html" target='_blank'>
                      <img src="./index_files/images/paper.png" height="80px"><br>
                          <h4><strong>Main Paper</strong></h4>
                      </a>
                  </li>
                  <!-- <li>
                      <a href="#video">
                      <img src="index_files/images/youtube_icon_dark.png" height="80px"><br>
                          <h4><strong>Video</strong></h4>
                      </a>
                  </li> -->
                  
                   <!-- <li>
                      <a href="#dataset">
                      <img src="./index_files/images/data.png" height="80px"><br>
                          <h4><strong>Data</strong></h4>
                      </a>
                  </li> -->
                  <li>
                      <a href="https://github.com/code-Porunacabeza/GAZER">
                      <img src="./index_files/images/github_pad.png" height="80px"><br>
                          <h4><strong>Code</strong></h4>
                      </a>
                  </li>
                  <li>
                      <a href="#BibTeX">
                      <img src="./index_files/images/bibtex.jpg" height="80px"><br>
                          <h4><strong>BibTeX</strong></h4>
                      </a>
                  </li>
              </ul>
          </div>
      </div>

      <div class="row">
          <div class="col-md-8 col-md-offset-2">
              <h3>
                  <strong>Overview</strong>
              </h3>
              <img src="./index_files/images/overreview_page-0001.jpg" class="img-responsive" alt="overview"><br>
              <p class="text-justify">
                Human-centered (HC) artificial intelligence (AI) collaborates human doctors and AI systems, that keeps human decision power (responsible) and improves their decision outcome, demonstrating great potential in medical visual assistance. During the interaction, it is able to assists the doctors to recognize the objects in medical images, thus enhancing their capabilities and empowering them achieve their clinical goals. Especially, with the development of foundation models, e.g., Segment Anything Models (SAM) and LLaVA , their emergence and homogenization capabilities enable the HCAI to be practiced in a wide range of medical scenarios through interaction. Therefore, it is poised to broadly reshape medical imaging and showing a promising future in energizing the capability of doctors. 
              </p>
          </div>
      </div>

      <!-- <div class="row" id="video">
          <div class="col-md-8 col-md-offset-2">
              <h3>
                  <strong>Video</strong>
              </h3>
              <p style="color:blue;font-size:11px;">(Waiting...)</p>
              <div class="text-center">
                  <video id="video_id" width="100%" controls="" controlsList="nodownload">>
                      <source src="./index_files/videos/video.mp4" type="video/mp4">
                      <track label="English" kind="subtitles" srclang="en" src="./index_files/videos/captions.vtt">
                  </video>

                  <script type="text/javascript">
                      $(document).ready(function() {
                      var video = document.querySelector('#video_id'); // get the video element
                      var tracks = video.textTracks; // one for each track element
                      var track = tracks[0]; // corresponds to the first track element
                      track.mode = 'hidden';});
                  </script>

              </div>
          </div>
      </div> -->
      
      <div class="row">
          <div class="col-md-8 col-md-offset-2">
              <h3>
                  <strong>Highlights</strong>
              </h3>
              <p class="text-justify">
                  <ul>
                      <li>
                        It provides an AI-driven human-AI collaboration that empower doctors' medical ability without interference with their clinical workflow.
                      </li>
                      <li>
                        We propose a novel gaze-prompted segment anything model, GAZER, for model-active interaction.
                      </li>
                      <li>
                        A plug-and-play gaze point filter (GPF) module that stimulates the gaze-prompt-based emergence ability of the foundation models without any additional training.
                      </li>
                      <li>
                        We present a gaze prompt learning (GPL) that learns to cognize the gaze maps and understand the human intentions in our GAZER, thus assisting medical visual practices in professional scenarios.
                      </li>
                  </ul>
              </p>
          </div>
      </div>


      <div class="row">
          <div class="col-md-8 col-md-offset-2">
              <h3>
                  <strong>GAZER is flexibly and seamlessly applied in a variety of clinical scenarios</strong>
              </h3>
              <div class="wrap">
                  <img src="./index_files/images/sen_page-0001.jpg" class="img-responsive" alt="overview">
                  <div class="txt">
                      <li>
                      Assist doctors in positioning and prompt doctors with relevant information during ultrasound scanning.
                      </li>
                      <li>
                      The lesion was located during the interventional procedure.
                      </li>
                      <li>
                      Observation inexperienced doctors gaze at the area for medical education.
                      </li>
                      <li>
                      Assist doctors in the location and diagnosis of multiple objects in the image.
                      </li>
                  </div>
              </div>
          </div>
      </div>

      <!-- <div class="row">
          <div class="col-md-8 col-md-offset-2">
              <h3>
                  <strong>Challenge</strong>
              </h3>
              <div class="wrap">
                  <div class="txtleft">
                      It is challenging to measure a reliable inter-image similarity.
                      <li>
                      Large appearance similarity between different semantic regions: the Myo and RA regions in images A and B.
                      </li>
                      <li>
                      Appearance dissimilarity between same semantic regions: RA regions are different in images B and C.
                      </li>
                  </div>
                  <img src="./index_files/images/chall.png" class="img-responsive" alt="overview">

              </div>
          </div>
      </div> -->


      <!-- <div class="row">
          <div class="col-md-8 col-md-offset-2">
              <h3>
                  <strong>Motivation</strong>
              </h3>
              <div class="wrap">
                  <img src="./index_files/images/fig.png" class="img-responsive" alt="overview">
                  <div class="txt">
                      The <strong style="color:rgb(204,0,0);">topological invariance</strong> of the visual semantics between the 3D medical images provides a motivation to discover their inter-image correspondence.
                  </div>
              </div>
          </div>
      </div> -->


      <div class="row">
          <div class="col-md-8 col-md-offset-2">
              <h3>
                  <strong>The framework of our GAZER</strong>
              </h3>
              <img src="./index_files/images/framework_page-0001.jpg" class="img-responsive" alt="overview">
              <p class="text-justify">
                  <li>
                    <strong >GAZER</strong>  embeds the gaze as the prompt into the SAM \cite{kirillov2023segment} architecture, for interactive semantic regions extraction on medical images according to the gaze. It has two modes (GAZER-GPF and GAZER-GPL) for the superiority of both universality and professionality.
                  </li>
                  <li>
                    <strong >GAZER-GPF</strong>  module encodes the gaze points into the model inference process, so that it is able to embed the gaze into the point prompt encoder of SAM framework without any additional training.
                  </li>
                  <li>
                    <strong >GAZER-GPL</strong>  trains a gaze map prompt encoder in the SAM's segmentation learning to understanding human intentions from their gaze maps, achieving reliable segmentation in professional medical scenarios.
                  </li>
              </p>
          </div>
      </div>

      <div class="row">
          <div class="col-md-8 col-md-offset-2">
              <h3>
                  <strong>GAZER-GPL is able to understand the distinction between different gaze maps</strong>
              </h3>
              <div class="wrap">
                  <div class="txtleft">
                    GAZER-GPL is able to better understand and distinguish different gaze maps as prompt. 
                    We obtain the prompt embeddings from different models prompt encoder, then  dimensionality reduction was applied via t-SNE and visualization display. With fine-tune on GAZER, we found that GAZER-GPL could still effectively distinguish between
                    various objects.
                  </div>
                  <img src="./index_files/images/feature_page-0001.jpg" class="img-responsive" alt="overview">
              </div>
          </div>
      </div>

      <div class="row" id="BibTeX">
          <div class="col-md-8 col-md-offset-2">
              <h3>
                  <strong>BibTeX</strong>
              </h3>
             If you find our project or pre-trained parameters useful in your research, please cite:
              <pre class="w3-panel w3-leftbar w3-light-grey" style="white-space: pre-wrap; font-family: monospace; font-size: 10px">
                  <!-- @InProceedings{He_2023_CVPR,
                    author    = {Yuting He, Guanyu Yang, Rongjun Ge, Yang Chen, Jean-Louis Coatrieux, Boyu Wang, Shuo Li},
                    title     = {Geometric Visual Similarity Learning in 3D Medical Image Self-supervised Pre-training},
                    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
                    month     = {June},
                    year      = {2023},
                    pages     = {}
                  } -->
                </pre>
          </div>
      </div>

      <!-- <div class="row">
          <div class="col-md-8 col-md-offset-2">
              <h3>
                  <strong>Acknowledgments</strong>
              </h3>
              The website template was borrowed from <a href="https://sggpoint.github.io">Chaoyi Zhang</a>.
              <p></p>
          </div>
      </div> -->
  </div>


</body></html>
